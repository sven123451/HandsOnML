{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes From Class\n",
    "\n",
    "Decomposition/Dimensionality Reduction Methods:\n",
    "\n",
    "    Projection methods: Like PCA (principal component analysis), representation of the input space as a lower dimension to simplify our analysis. Think of it as denoise-ing, simplifying, we want to PRESERVE AS MUCH OF THE VARIANCE of the data as we can while compressing/simplifying our data by lowering the dimension whether that be 3-2, 2-1 etc.\n",
    "\n",
    "    Kernel Methods: Kernel spaces are trying to find a higher dimensional space where the problem is simpler to deconstruct. Imagine in a 2 dimensional space we cannot discern the difference between points, but if we lift it into a 3d space, the verticality differentiates these points thus making the problem simpler. Like Kernel PCA or SVM (support vector machines)\n",
    "\n",
    "    Manifold methods: Manifold methods assume that a high-dimensional dataset actually lies on or near a low-dimensional manifold (a curved surface embedded in high dimensional space. Goal is to find a low dimensional space that better represents how we measure similarity in our data by unfolding or flattening the manifold.\n",
    "\n",
    "\n",
    "PCA In detail:\n",
    "\n",
    "    Principal components are the new axes in the data space that are orthogonal (uncorrelated) to each other. Each principal component is a linear combination of original features. You get a matrix with the variation of each transformation/translatation of your data, first principal component is the component that accounts for the most variation in the data. We can drop components that account for the least amount of variation in the data thus compression/reducing our data while preserving value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Each Principal component is a linear transformation/transalation of the original features. So PC1 is not a single feature/dimension of the dataset, it is a linear model of all features weighted differently. PCA is often used as a preprocessing step, reduce dimensions before you build your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction menas reducing the number of input features for your model to improve model training faster and make it easier to find a good solution to your problem. Reducing dimensionality does cause some information loss, just like compressing an image to JPEG can degrade its quality, so even though it will speed up training, it may make your system perform slightly worse. In some cases, reducing the dimensionality of the training data may filter out some noise and unnecessary details and improve performance, but it general it won't; it will just speed up training.\n",
    "\n",
    "MNIST Example: Most of the data in the MNIST dataset are images where the information is all contained in a small amount of the pixels, we only really use some of the dimensions (pixels) in our analysis, meaning there is a lot of redundancy. Dimensionality reduction can help with this by taking us from a high dimensional space to a low dimensional space which still captures most of the information in our data.\n",
    "\n",
    "Apart from speeding up training, dimensionality reduction is extremely useful for data visualization. Reducing the number of dimensions down to two/three makes it possible to plot a condensed view of a high-dimensional training set on a graph and gain insights by visually detecting patterns, such as clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
