{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Computer Vision using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Notes (W7 Pred analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Deep Neural Network over Shallow Network:\n",
    "\n",
    "Parameter Efficiency:\n",
    "\n",
    "    Deep networks more efficiently represent a higher order function\n",
    "\n",
    "    They can achieve the same performance as shallow networks with fewer total parameters\n",
    "\n",
    "Hierarchical Feature Learning:\n",
    "\n",
    "    Deep networks learn features at multiple levels of abstraction\n",
    "\n",
    "    Lower layers learn simple features like edges/textures\n",
    "\n",
    "    Higher layer combine these into complex concepts (objects, images)\n",
    "\n",
    "Smoother Loss Surface:\n",
    "\n",
    "    For reasons we don't fully understand, deep neural networks can find better generalizations because thye are creating/finding more stable minima than the shallow network. This means you can find solutions that are more representative of the variation you see in the test data. \n",
    "\n",
    "Decrease in error after parameters exceed data points\n",
    "\n",
    "    If we have more parameters than the model than data, we have a smoothing or implicit regularization effect that results in better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Activation Functions:\n",
    "\n",
    "Crucial in neural networks for several key reasons:\n",
    "\n",
    "1. Introducing Non-linearity\n",
    "    \n",
    "    Without activation functions, neural networks would just be linear transformations\n",
    "\n",
    "    Non-linearity allows networks to learn complex patterns and relationships\n",
    "\n",
    "2. Feature Transformation\n",
    "\n",
    "    Transforms inputs into more useful representations\n",
    "\n",
    "    Maps inputs to different ranges (e.g, probabilities between 0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Technique used to prevent exploding gradients by limiting the maximum value gradients can take\n",
    "\n",
    "At each point on a loss surface you may get differing gradient information in different directions, momentum might deal with some, but we need to deal with big gradients with clipping.\n",
    "\n",
    "How it works:\n",
    "\n",
    "    1. Calculate gradients normally during backpropagation\n",
    "\n",
    "    2. Before applying gradients, clip them:\n",
    "\n",
    "        Clip by value: Cap each individual gradient element to a threshold\n",
    "\n",
    "        Clip by norm: Scale the entire gradient vector if its norm exceeds a threshold\n",
    "\n",
    "By combining momentum (for handling varying gradient directions and plateaus) with clipping (for handling extreme gradient magnitudes), we get more stable and effective training across the complex loss landscapes of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "Think of it as calculating the derivative of every weight for each layer on the backward pass. It is just applying gradient descent for a neural netwrok, we need a minimum of derivatives for all weights to determine the direction to move.\n",
    "\n",
    "Once we calculate gradients starting from the output layer and working backwards, we can move the weights in the direction that reduces error. In this case the learning rate controls how big of a step to take.\n",
    "\n",
    "The end goal is always to minimise the loss function for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricks to make Neural Networks work\n",
    "\n",
    "Dropout: Different versions of loss-surface\n",
    "\n",
    "HE Initialisation (Another hyperparameter): When initialising weights we can fix the variance of the random distribution we used for the weights. It solves the issue of weight variance being overexaggerated on the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "Specialized neural networks designed primarily for processing grid-like data such as images. Essentially just layers of convolutional (filter) layers and pooling layers repeated and then an MLP (multilayer perceptron) on top before output. The advantage of this is that image data can have too large of an input space (especially if it is too high quality), filter layers can remove redundancies in the input space and reduce the spatial dimensions of our inputs.\n",
    "\n",
    "Because of sparsity of connections of the convolutional layers, we force localisation by limiting the neurons that the layer in front can learn about (simpler features).\n",
    "\n",
    "The further you get from the input signal, the harder the model becomes to interpret. Convolutional/Pooling layers easier to interpret than the fully connected MLP that sits on top.\n",
    "\n",
    "1. Convolutional Layers\n",
    "\n",
    "        Essentially feature extraction\n",
    "\n",
    "        Apply filters that scan across the input, detects specific patterns (edges etc.)\n",
    "\n",
    "        Each convolutional layer typically has many filters (32, 64)\n",
    "\n",
    "        Each filter creates one feature map in the output\n",
    "\n",
    "        More filters = more patterns detected\n",
    "\n",
    "        Share weights across the entire image (parameter efficiency)\n",
    "\n",
    "        Preserve spatial relationships in the data\n",
    "\n",
    "2. Pooling Layers\n",
    "\n",
    "        Image classification is often Massively overparametised because of the size of the input space, pooling layers reduce these parameter redundancy.\n",
    "        \n",
    "        In a sense, what we are interested in is very small compared to the whole input space (pixels of an image)\n",
    "\n",
    "        Reduce spatial dimensions (downsampling)\n",
    "\n",
    "        Take the original output of CL (featurebank) and downsizing\n",
    "\n",
    "        Make the network less sensitive to exact positions\n",
    "\n",
    "        Help with Computational efficiency\n",
    "\n",
    "        \n",
    "\n",
    "3. Fully Connected layers\n",
    "\n",
    "        Usually placed after convolutional/pooling layers\n",
    "\n",
    "        Process high level features extracted by convolutions\n",
    "\n",
    "        Make final predictions based on these features\n",
    "\n",
    "4. Hierarchical Learning\n",
    "\n",
    "        Each layer builds upon previous layers like so:\n",
    "\n",
    "        Pixels → Edges → Textures → Patterns → Parts → Objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections in Neural Networks\n",
    "\n",
    "#### Basic Structure\n",
    "- Skip connections are local shortcuts in the network\n",
    "- They jump over a few layers at a time\n",
    "- Multiple skip connections exist at different points\n",
    "- They don't skip to the final output\n",
    "\n",
    "#### Information Flow\n",
    "1. Input enters a network block\n",
    "2. Information splits into two paths:\n",
    "   - Main path: Through several layers\n",
    "   - Skip path: Bypasses these layers\n",
    "3. At the connection point:\n",
    "   - Main path output and skip path meet\n",
    "   - Values are added together\n",
    "   - Combined result feeds into next block\n",
    "4. Process repeats through network\n",
    "\n",
    "#### Building Analogy\n",
    "Think of a building where:\n",
    "- Regular layers = Taking stairs between floors\n",
    "- Skip connections = Express elevators\n",
    "- You don't have one elevator from lobby to penthouse\n",
    "- Instead, you have multiple express elevators:\n",
    "  - Ground to Floor 3\n",
    "  - Floor 3 to Floor 6\n",
    "  - Floor 6 to Floor 9\n",
    "  - And so on...\n",
    "\n",
    "This creates a network with multiple \"local bypasses\" rather than one big skip to the end. Smoothes and widens the loss surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Neural Networks & Signal Sampling\n",
    "\n",
    "#### Nyquist Rate Fundamentals\n",
    "- Minimum sampling rate required to accurately capture a signal\n",
    "- Must sample at least 2x the highest frequency component\n",
    "- Nyquist frequency = highest frequency that can be accurately represented\n",
    "- Sampling frequency must be > 2 * highest signal frequency\n",
    "\n",
    "#### Determining Sampling Rate\n",
    "1. Identify highest frequency component (fmax)\n",
    "   - Through signal analysis or domain knowledge\n",
    "   - Consider all important frequencies in your data\n",
    "   - Account for noise frequencies if relevant\n",
    "\n",
    "2. Calculate minimum sampling rate\n",
    "   - Minimum sampling frequency = 2 * fmax\n",
    "   - In practice, use 2.5x to 4x for safety margin\n",
    "   - Example: \n",
    "     - Signal frequency = 100 Hz\n",
    "     - Minimum sampling = 200 Hz\n",
    "     - Recommended sampling ≈ 250-400 Hz\n",
    "\n",
    "#### Time Between Samples\n",
    "1. Basic calculation:\n",
    "   - Time interval = 1/sampling_frequency\n",
    "   - Example: 400 Hz sampling → 1/400 = 0.0025 seconds between samples\n",
    "\n",
    "2. Practical considerations:\n",
    "   - System capabilities (sensors, processing)\n",
    "   - Storage constraints\n",
    "   - Real-time requirements\n",
    "   - Signal variability\n",
    "\n",
    "#### Time Series NN Architecture Choices\n",
    "1. Input window size:\n",
    "   - Must capture relevant temporal patterns\n",
    "   - Should cover multiple cycles of lowest frequency\n",
    "   - Consider domain-specific time dependencies\n",
    "\n",
    "2. Common architectures:\n",
    "   - RNN/LSTM for sequential data\n",
    "   - 1D CNN for pattern detection\n",
    "   - Transformer for long-range dependencies\n",
    "   - Hybrid models for complex patterns\n",
    "\n",
    "3. Output considerations:\n",
    "   - Single-step vs multi-step prediction\n",
    "   - Classification vs regression\n",
    "   - Real-time requirements\n",
    "\n",
    "#### Common Pitfalls\n",
    "- Undersampling (aliasing)\n",
    "- Oversampling (computational waste)\n",
    "- Irregular sampling intervals\n",
    "- Missing data handling\n",
    "- Non-stationary signals\n",
    "\n",
    "#### Best Practices\n",
    "1. Signal analysis:\n",
    "   - Perform frequency analysis before deciding rate\n",
    "   - Consider all frequency components\n",
    "   - Account for future signal changes\n",
    "\n",
    "2. Implementation:\n",
    "   - Use consistent sampling intervals\n",
    "   - Handle missing data appropriately\n",
    "   - Monitor for signal drift\n",
    "   - Validate sampling adequacy\n",
    "\n",
    "3. Validation:\n",
    "   - Check reconstruction quality\n",
    "   - Verify no information loss\n",
    "   - Test with different conditions\n",
    "   - Monitor system performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook Notes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
